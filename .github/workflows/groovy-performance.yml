# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Performance Benchmarks

# Run on demand, scheduled weekly, or on performance-related PRs
on:
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark group to run (e.g., dispatch, orm). Leave empty for all groups.'
        required: false
        default: ''
      compare_baseline:
        description: 'Compare against baseline'
        required: false
        default: 'false'
        type: boolean
  schedule:
    # Run every Sunday at 2am UTC
    - cron: '0 2 * * 0'
  pull_request:
    paths:
      - 'src/main/**/runtime/**'
      - 'src/main/**/vmplugin/**'
      - 'subprojects/performance/**'
  push:
    paths:
      - 'src/main/**/runtime/**'
      - 'src/main/**/vmplugin/**'
      - 'subprojects/performance/**'

permissions:
  contents: read

env:
  DEVELOCITY_ACCESS_KEY: ${{ secrets.DEVELOCITY_ACCESS_KEY }}

jobs:
  # Quick smoke test for PRs
  performance-smoke:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'
      - uses: gradle/actions/setup-gradle@v4

      - name: Run smoke benchmarks
        run: |
          ./gradlew -Pindy=true -PbenchInclude=ColdCall :performance:jmh \
            -Pjmh.fork=1 -Pjmh.wi=2 -Pjmh.i=3
        timeout-minutes: 30

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-smoke-results
          path: subprojects/performance/build/results/jmh/

  # ============================================================================
  # Full benchmark suite: build once, fan out into parallel matrix jobs
  # ============================================================================

  # Step 1: Build the JMH fat jar once
  build-jmh-jar:
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'
      - uses: gradle/actions/setup-gradle@v4

      - name: Build JMH fat jar
        run: ./gradlew :performance:jmhJar

      - name: Upload JMH jar
        uses: actions/upload-artifact@v4
        with:
          name: jmh-jar
          path: subprojects/performance/build/libs/*-jmh.jar
          retention-days: 1

  # Step 2: Read benchmark groups and output matrix JSON
  discover-groups:
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4

      - name: Build matrix from benchmark groups
        id: set-matrix
        run: |
          FILTER="${{ github.event.inputs.benchmark_filter }}"
          if [ -n "$FILTER" ]; then
            # Run only the requested group
            MATRIX=$(jq -c --arg f "$FILTER" '[.[] | select(.group == $f)]' .github/benchmark-groups.json)
          else
            MATRIX=$(jq -c '.' .github/benchmark-groups.json)
          fi
          echo "matrix={\"include\":$(echo "$MATRIX" | jq -c '[.[] | {group: .group, pattern: .pattern, indy: true}] + [.[] | {group: .group, pattern: .pattern, indy: false}]')}" >> "$GITHUB_OUTPUT"

  # Step 3: Run benchmarks in parallel (groups x indy modes)
  benchmark-matrix:
    needs: [build-jmh-jar, discover-groups]
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.discover-groups.outputs.matrix) }}
    steps:
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'

      - name: Download JMH jar
        uses: actions/download-artifact@v4
        with:
          name: jmh-jar
          path: .

      - name: Run benchmarks (${{ matrix.group }}, indy=${{ matrix.indy }})
        run: |
          JAR=$(ls *-jmh.jar | head -1)
          java \
            -Dgroovy.target.indy=${{ matrix.indy }} \
            -jar "$JAR" \
            "${{ matrix.pattern }}" \
            -wi 1 \
            -rf json -rff results.json
        timeout-minutes: 45

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bench-${{ matrix.group }}-indy-${{ matrix.indy }}
          path: results.json
          retention-days: 5

  # Step 4: Collect all results and generate comparison report
  collect-and-compare:
    needs: benchmark-matrix
    if: always() && github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: bench-*
          path: results/
          merge-multiple: false

      - name: Merge results and generate comparison
        run: |
          python3 << 'PYEOF'
          import json, os, sys
          from pathlib import Path

          indy_results = {}
          noindy_results = {}

          results_dir = Path("results")
          for artifact_dir in sorted(results_dir.iterdir()):
              if not artifact_dir.is_dir():
                  continue
              results_file = artifact_dir / "results.json"
              if not results_file.exists():
                  print(f"Warning: no results.json in {artifact_dir.name}", file=sys.stderr)
                  continue

              is_indy = "-indy-true" in artifact_dir.name
              target = indy_results if is_indy else noindy_results

              with open(results_file) as f:
                  data = json.load(f)

              for bench in data:
                  name = bench.get("benchmark", "unknown")
                  score = bench.get("primaryMetric", {}).get("score", 0)
                  unit = bench.get("primaryMetric", {}).get("scoreUnit", "")
                  error = bench.get("primaryMetric", {}).get("scoreError", 0)
                  target[name] = {"score": score, "unit": unit, "error": error}

          # Save merged results
          with open("indy-results.json", "w") as f:
              json.dump(indy_results, f, indent=2)
          with open("noindy-results.json", "w") as f:
              json.dump(noindy_results, f, indent=2)

          # Generate comparison report
          all_benchmarks = sorted(set(list(indy_results.keys()) + list(noindy_results.keys())))

          lines = []
          lines.append("## Performance Comparison: Indy vs Non-Indy")
          lines.append("")
          lines.append(f"| Benchmark | Indy | Non-Indy | Diff |")
          lines.append("|-----------|------|----------|------|")

          regressions = []
          improvements = []

          for name in all_benchmarks:
              short_name = name.split(".")[-1] if "." in name else name
              indy = indy_results.get(name)
              noindy = noindy_results.get(name)

              if indy and noindy and noindy["score"] > 0:
                  diff_pct = ((indy["score"] - noindy["score"]) / noindy["score"]) * 100
                  diff_str = f"{diff_pct:+.1f}%"
                  if diff_pct > 10:
                      diff_str += " :arrow_up:"
                      improvements.append((short_name, diff_pct))
                  elif diff_pct < -10:
                      diff_str += " :arrow_down:"
                      regressions.append((short_name, diff_pct))
                  lines.append(f"| {short_name} | {indy['score']:.3f} {indy['unit']} | {noindy['score']:.3f} {noindy['unit']} | {diff_str} |")
              elif indy:
                  lines.append(f"| {short_name} | {indy['score']:.3f} {indy['unit']} | N/A | - |")
              elif noindy:
                  lines.append(f"| {short_name} | N/A | {noindy['score']:.3f} {noindy['unit']} | - |")

          lines.append("")
          lines.append(f"**Total benchmarks:** {len(all_benchmarks)} | "
                       f"**Indy faster (>10%):** {len(improvements)} | "
                       f"**Non-Indy faster (>10%):** {len(regressions)}")

          report = "\n".join(lines)

          with open("comparison-report.md", "w") as f:
              f.write(report)

          # Write to GitHub Step Summary
          summary_path = os.environ.get("GITHUB_STEP_SUMMARY", "")
          if summary_path:
              with open(summary_path, "a") as f:
                  f.write(report + "\n")

          print(report)
          PYEOF

      - name: Upload comparison report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-comparison
          path: |
            comparison-report.md
            indy-results.json
            noindy-results.json
          retention-days: 30

  # Memory-focused benchmarks with GC profiler (parallel with main matrix)
  performance-memory:
    needs: build-jmh-jar
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'

      - name: Download JMH jar
        uses: actions/download-artifact@v4
        with:
          name: jmh-jar
          path: .

      - name: Run memory benchmarks with GC profiler
        run: |
          JAR=$(ls *-jmh.jar | head -1)
          java \
            -Dgroovy.target.indy=true \
            -jar "$JAR" \
            ".*bench\.memory\..*" \
            -f 1 -wi 1 -i 1 -r 2s -w 2s \
            -prof gc \
            -rf json -rff gc-profile-results.json

      - name: Upload memory profile results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-memory-profile
          path: gc-profile-results.json

  # Threshold sweep analysis
  performance-threshold-sweep:
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'
      - uses: gradle/actions/setup-gradle@v4

      - name: Run threshold sweep
        run: |
          ./gradlew -PbenchInclude=Warmup :performance:jmhThresholdSweep
        timeout-minutes: 180

      - name: Upload threshold sweep results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-threshold-sweep
          path: subprojects/performance/build/results/jmh-compare/threshold-sweep/

      - name: Display threshold summary
        run: |
          echo "## Threshold Sweep Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f subprojects/performance/build/results/jmh-compare/threshold-sweep/threshold-summary.txt ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat subprojects/performance/build/results/jmh-compare/threshold-sweep/threshold-summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
